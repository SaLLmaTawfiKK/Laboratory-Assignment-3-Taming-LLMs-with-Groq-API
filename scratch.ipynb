{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import groq\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        load_dotenv(\"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\")\n",
    "        # Assign the GROQ_API_KEY environment variable to self.api_key\n",
    "        self.api_key = os.environ.get(\"GROQ_API_KEY\")  \n",
    "        if not self.api_key:\n",
    "            print(\"Error: GROQ_API_KEY not found in .env file\")\n",
    "            print(\"Current working directory:\", os.getcwd())\n",
    "            print(\".env file exists:\", os.path.exists(\".env\"))\n",
    "            if os.path.exists(\".env\"):\n",
    "                print(\".env contents (first line):\", open(\".env\").readline().strip())\n",
    "            raise ValueError(\"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\")\n",
    "        try:\n",
    "            self.client = Groq(api_key=self.api_key)\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama3-70b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "                max_tokens=10\n",
    "            )\n",
    "            print(\"API key validated successfully:\", response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error validating API key: {str(e)}\")\n",
    "            raise\n",
    "        self.model = \"llama3-70b-8192\"\n",
    "\n",
    "    def complete(self, prompt, max_tokens=1000, temperature=0.7, retries=3):\n",
    "        \"\"\"\n",
    "        Get completion from Groq API with retry logic for rate limits and errors\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                print(f\"API Response for prompt '{prompt[:50]}...':\", response)\n",
    "                return response.choices[0].message.content\n",
    "            except groq.RateLimitError as e:\n",
    "                print(f\"Rate limit hit. Waiting before retry {attempt + 1}/{retries}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in completion: {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    return None\n",
    "                time.sleep(1)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Structured Completions\n",
    "def create_structured_prompt(text, question):\n",
    "    \"\"\"\n",
    "    Creates a structured prompt that will produce a completion with\n",
    "    easily recognizable sections.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "# Analysis Report\n",
    "## Input Text\n",
    "{text}\n",
    "## Question\n",
    "{question}\n",
    "## Analysis\n",
    "\"\"\"\n",
    "    print(\"Structured prompt created:\", prompt)\n",
    "    return prompt\n",
    "\n",
    "def extract_section(completion, section_start, section_end=None):\n",
    "    \"\"\"\n",
    "    Extracts content between section_start and section_end.\n",
    "    If section_end is None, extracts until the end of the completion.\n",
    "    \"\"\"\n",
    "    start_idx = completion.find(section_start)\n",
    "    if start_idx == -1:\n",
    "        print(f\"Section '{section_start}' not found in completion\")\n",
    "        return None\n",
    "    start_idx += len(section_start)\n",
    "    if section_end is None:\n",
    "        result = completion[start_idx:].strip()\n",
    "    else:\n",
    "        end_idx = completion.find(section_end, start_idx)\n",
    "        if end_idx == -1:\n",
    "            result = completion[start_idx:].strip()\n",
    "        else:\n",
    "            result = completion[start_idx:end_idx].strip()\n",
    "    print(f\"Extracted section from '{section_start}':\", result)\n",
    "    return result\n",
    "\n",
    "def stream_until_marker(client, prompt, stop_marker, max_tokens=1000):\n",
    "    \"\"\"\n",
    "    Streams the completion and stops once a marker is detected.\n",
    "    Returns the accumulated text up to the marker.\n",
    "    \"\"\"\n",
    "    print(f\"Starting streaming for prompt '{prompt[:50]}...' with stop marker '{stop_marker}'\")\n",
    "    accumulated_text = \"\"\n",
    "    try:\n",
    "        stream = client.client.chat.completions.create(\n",
    "            model=client.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content or \"\"\n",
    "            accumulated_text += content\n",
    "            print(f\"Streaming chunk received:\", content)\n",
    "            if stop_marker in accumulated_text:\n",
    "                end_idx = accumulated_text.find(stop_marker)\n",
    "                result = accumulated_text[:end_idx].strip()\n",
    "                print(f\"Stop marker found. Streaming result:\", result)\n",
    "                return result\n",
    "        print(\"Streaming completed without finding stop marker\")\n",
    "        return accumulated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Streaming error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Classification with Confidence Analysis\n",
    "def classify_with_confidence(client, text, categories, confidence_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Classifies text into one of the provided categories.\n",
    "    Returns the classification only if confidence is above threshold.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Classify the following text into exactly one of these categories: {', '.join(categories)}.\n",
    "\n",
    "Response format:\n",
    "1. CATEGORY: [one of: {', '.join(categories)}]\n",
    "2. CONFIDENCE: [high|medium|low]\n",
    "3. REASONING: [explanation]\n",
    "\n",
    "Text to classify:\n",
    "{text}\n",
    "\"\"\"\n",
    "    print(f\"Classification prompt for '{text}':\", prompt)\n",
    "    \n",
    "    try:\n",
    "        response = client.client.chat.completions.create(\n",
    "            model=client.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=500,\n",
    "            temperature=0\n",
    "        )\n",
    "        print(f\"API Response for classification of '{text}':\", response)\n",
    "        completion = response.choices[0].message.content\n",
    "        category = extract_section(completion, \"1. CATEGORY: \", \"\\n\")\n",
    "        reasoning = extract_section(completion, \"3. REASONING: \", \"\\n\")\n",
    "        confidence_level = extract_section(completion, \"2. CONFIDENCE: \", \"\\n\")\n",
    "        \n",
    "        confidence_score = 0.9 if confidence_level == \"high\" else 0.6 if confidence_level == \"medium\" else 0.3\n",
    "        \n",
    "        result = {\n",
    "            \"category\": category,\n",
    "            \"confidence\": confidence_score,\n",
    "            \"reasoning\": reasoning\n",
    "        }\n",
    "        print(f\"Classification result for '{text}':\", result)\n",
    "        \n",
    "        if confidence_score > confidence_threshold:\n",
    "            return result\n",
    "        else:\n",
    "            return {\n",
    "                \"category\": \"uncertain\",\n",
    "                \"confidence\": confidence_score,\n",
    "                \"reasoning\": \"Confidence below threshold\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Classification error for '{text}': {str(e)}\")\n",
    "        return {\n",
    "            \"category\": \"error\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": f\"Error during classification: {str(e)}\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Prompt Strategy Comparison\n",
    "def compare_prompt_strategies(client, texts, categories):\n",
    "    \"\"\"\n",
    "    Compares different prompt strategies on the same classification tasks.\n",
    "    \"\"\"\n",
    "    print(\"\\nComparison of Prompt Strategies:\")\n",
    "    strategies = {\n",
    "        \"basic\": lambda text: f\"Classify this text into one of these categories: {', '.join(categories)}\\nText: {text}\\nClassification:\",\n",
    "        \"structured\": lambda text: f\"\"\"\n",
    "Classification Task\n",
    "Categories: {', '.join(categories)}\n",
    "Text: {text}\n",
    "Classification:\"\"\",\n",
    "        \"few_shot\": lambda text: f\"\"\"\n",
    "Here are some examples of text classification:\n",
    "Example 1:\n",
    "Text: \"The product arrived damaged and customer service was unhelpful.\"\n",
    "Classification: negative\n",
    "Example 2:\n",
    "Text: \"While delivery was slow, the quality exceeded my expectations.\"\n",
    "Classification: neutral\n",
    "Example 3:\n",
    "Text: \"Absolutely love this! Best purchase I've made all year.\"\n",
    "Classification: positive\n",
    "Now classify this text:\n",
    "Text: \"{text}\"\n",
    "Classification:\"\"\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for strategy_name, prompt_func in strategies.items():\n",
    "        print(f\"\\nStrategy: {strategy_name}\")\n",
    "        strategy_results = []\n",
    "        for text in texts:\n",
    "            start_time = time.time()\n",
    "            prompt = prompt_func(text)\n",
    "            print(f\"Prompt for '{text}':\", prompt)\n",
    "            try:\n",
    "                response = client.client.chat.completions.create(\n",
    "                    model=client.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=500,\n",
    "                    temperature=0\n",
    "                )\n",
    "                print(f\"API Response for '{text}':\", response)\n",
    "                completion = response.choices[0].message.content\n",
    "                classification = completion.split(\"Classification:\")[-1].strip() if \"Classification:\" in completion else \"unknown\"\n",
    "                result = classify_with_confidence(client, text, categories)\n",
    "                classification = result[\"category\"]\n",
    "                confidence = result[\"confidence\"]\n",
    "                reasoning = result[\"reasoning\"]\n",
    "            except Exception as e:\n",
    "                print(f\"Error in strategy {strategy_name} for text '{text}': {str(e)}\")\n",
    "                strategy_results.append({\n",
    "                    \"text\": text,\n",
    "                    \"classification\": \"error\",\n",
    "                    \"time\": 0,\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"reasoning\": \"API error\"\n",
    "                })\n",
    "                continue\n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "            \n",
    "            strategy_results.append({\n",
    "                \"text\": text,\n",
    "                \"classification\": classification,\n",
    "                \"time\": time_taken,\n",
    "                \"confidence\": confidence,\n",
    "                \"reasoning\": reasoning\n",
    "            })\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Classification: {classification}\")\n",
    "            print(f\"Confidence: {confidence}\")\n",
    "            print(f\"Reasoning: {reasoning}\")\n",
    "            print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "        results[strategy_name] = strategy_results\n",
    "    \n",
    "    # Text-based comparison instead of visualization\n",
    "    print(\"\\nStrategy Comparison Summary:\")\n",
    "    for strategy, data in results.items():\n",
    "        print(f\"\\nStrategy: {strategy}\")\n",
    "        category_counts = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"error\": 0, \"uncertain\": 0}\n",
    "        total_time = 0\n",
    "        for result in data:\n",
    "            category_counts[result[\"classification\"]] = category_counts.get(result[\"classification\"], 0) + 1\n",
    "            total_time += result[\"time\"]\n",
    "        avg_time = total_time / len(data) if data else 0\n",
    "        print(\"Classification Distribution:\")\n",
    "        for cat, count in category_counts.items():\n",
    "            print(f\"  {cat}: {count}\")\n",
    "        print(f\"Average Response Time: {avg_time:.2f} seconds\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  try:\n",
    "      print(\"Starting execution of taming_llm.py\")\n",
    "      client = LLMClient() # Test Part 1: Basic Completion \n",
    "      print(\"\\nTesting Part 1: Basic Completion\")\n",
    "      test_prompt = \"Hello, how are you?\"\n",
    "      result = client.complete(test_prompt)\n",
    "      # Test Part 2: Structured Completions\n",
    "      print(\"Basic completion result:\", result)\n",
    "      print(\"\\nTesting Part 2: Structured Completions\")\n",
    "      test_text = \"The product was amazing and delivery was fast\"\n",
    "      test_question = \"What is the sentiment of this review?\"\n",
    "      prompt = create_structured_prompt(test_text, test_question)\n",
    "      completion = client.complete(prompt)\n",
    "      print(\"Full completion:\", completion)\n",
    "      analysis = extract_section(completion, \"## Analysis\\n\")\n",
    "      print(\"Extracted analysis:\", analysis)\n",
    "      streaming_result = stream_until_marker(client, prompt, \"END\")\n",
    "      print(\"Streaming result:\", streaming_result)# Test Part 3: Classification\n",
    "      print(\"\\nTesting Part 3: Classification\")\n",
    "      categories = [\"positive\", \"negative\", \"neutral\"]\n",
    "      result = classify_with_confidence(client, test_text, categories)\n",
    "      print(\"Classification result:\")\n",
    "      print(f\"Category: {result['category']}\")\n",
    "      print(f\"Confidence: {result['confidence']}\")\n",
    "      print(f\"Reasoning: {result['reasoning']}\")\n",
    "      # Test Part 4: Prompt Strategies\n",
    "      print(\"\\nTesting Part 4: Prompt Strategies\")\n",
    "      test_texts = [\"The product was amazing and delivery was fast\",\n",
    "                    \"Worst experience ever with customer service\", \n",
    "                    \"Quality was decent for the price\"]\n",
    "      results = compare_prompt_strategies(client, test_texts, categories)\n",
    "      print(\"\\nFinal Strategy Results:\")\n",
    "      for strategy, data in results.items():\n",
    "        print(f\"\\nStrategy: {strategy}\")\n",
    "        for result in data:\n",
    "          print(f\"Text: {result['text']}\")\n",
    "          print(f\"Classification: {result['classification']}\")\n",
    "          print(f\"Confidence: {result['confidence']}\")\n",
    "          print(f\"Reasoning: {result['reasoning']}\")\n",
    "          print(f\"Time: {result['time']:.2f} seconds\")\n",
    "      print(\"\\nExecution completed successfully\")\n",
    "  except Exception as e:\n",
    "      print(f\"Error in main execution: {str(e)}\")\n",
    "      raise\n",
    "  result = client.complete(test_prompt)\n",
    "  # Test Part 2: Structured Completions\n",
    "  print(\"Basic completion result:\", result)\n",
    "  print(\"\\nTesting Part 2: Structured Completions\")\n",
    "  test_text = \"The product was amazing and delivery was fast\"\n",
    "  test_question = \"What is the sentiment of this review?\"\n",
    "  prompt = create_structured_prompt(test_text, test_question)\n",
    "  completion = client.complete(prompt)\n",
    "  print(\"Full completion:\", completion)\n",
    "  analysis = extract_section(completion, \"## Analysis\\n\")\n",
    "  print(\"Extracted analysis:\", analysis)\n",
    "  streaming_result = stream_until_marker(client, prompt, \"END\")\n",
    "  print(\"Streaming result:\", streaming_result)# Test Part 3: Classification\n",
    "  print(\"\\nTesting Part 3: Classification\")\n",
    "  categories = [\"positive\", \"negative\", \"neutral\"]\n",
    "  result = classify_with_confidence(client, test_text, categories)\n",
    "  print(\"Classification result:\")\n",
    "  print(f\"Category: {result['category']}\")\n",
    "  print(f\"Confidence: {result['confidence']}\")\n",
    "  print(f\"Reasoning: {result['reasoning']}\")\n",
    "  # Test Part 4: Prompt Strategies\n",
    "  print(\"\\nTesting Part 4: Prompt Strategies\")\n",
    "  test_texts = [\"The product was amazing and delivery was fast\",\n",
    "                \"Worst experience ever with customer service\", \n",
    "                \"Quality was decent for the price\"]\n",
    "  results = compare_prompt_strategies(client, test_texts, categories)\n",
    "  print(\"\\nFinal Strategy Results:\")\n",
    "  for strategy, data in results.items():\n",
    "    print(f\"\\nStrategy: {strategy}\")\n",
    "    for result in data:\n",
    "      print(f\"Text: {result['text']}\")\n",
    "      print(f\"Classification: {result['classification']}\")\n",
    "      print(f\"Confidence: {result['confidence']}\")\n",
    "      print(f\"Reasoning: {result['reasoning']}\")\n",
    "      print(f\"Time: {result['time']:.2f} seconds\")\n",
    "      print(\"\\nExecution completed successfully\")\n",
    "  except Exception as e:\n",
    "  print(f\"Error in main execution: {str(e)}\")\n",
    "  raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
